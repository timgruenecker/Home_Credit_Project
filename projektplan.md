# ğŸ§  Home Credit Default Risk â€“ Projektplan (Portfolio-Version)

## ğŸ“Œ Ziel
Ein strukturiertes, modernes Machine-Learning-Projekt zur Vorhersage von KreditausfÃ¤llen (Home Credit Default Risk), basierend auf dem bekannten Kaggle-Wettbewerb.  
Ziel ist eine **nachvollziehbare Modellpipeline**, **leistungsstarke Vorhersagemodelle** und der Einsatz von **branchenrelevanten Standards** wie Stacking, SHAP, Docker und Reproduzierbarkeit.

---

## ğŸš€ Aktueller Stand (Ausgangspunkt)

- âœ… Notebook-basiertes Rumprobieren mit ~0.78 ROC AUC
- âœ… Eigenes Feature Engineering (keine fertigen Kernels)
- âœ… LightGBM, Grid/Random Search, AUC-Drop-Feature-Auswahl
- âŒ Keine Strukturierung, Wiederverwendbarkeit oder klare Pipeline
- âŒ Kein README, kein Deployment, keine Visualisierung, kein Stack

---

## ğŸ”§ Phase 1 â€“ Strukturierung & AufrÃ¤umen (1â€“2 Tage)

### 1.1 Projektstruktur (2h)
- Erstelle standardisierte Ordnerstruktur:

project/
â”œâ”€â”€ data/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ src/
â”œâ”€â”€ models/
â”œâ”€â”€ outputs/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md


### 1.2 Feature Engineering modularisieren (4â€“5h)
- `src/feature_engineering.py`
- Aufteilung nach Kategorien (z.â€¯B. VerhÃ¤ltnisfeatures, Flags, Summen)
- Saubere `make_features()`-Pipeline

### 1.3 Erste README.md (2â€“3h)
- Projektziel
- Wettbewerbsbeschreibung
- Aktueller Stand (Modell, Score)
- Ãœberblick Ã¼ber den Code
- Geplante Schritte (aus diesem Plan)

---

## ğŸ“ˆ Phase 2 â€“ Modell & Evaluation verbessern (3â€“5 Tage)

### 2.1 Evaluation erweitern (1 Tag)
- `src/evaluation.py`
- ROC AUC, ROC-Kurve, Confusion Matrix
- Threshold-Optimierung (Precision@k, F1)

### 2.2 Feature Selection systematisieren (1â€“2 Tage)
- AUC-Drop automatisieren
- SHAP Summary Plot & Feature Filtering
- Korrelationen prÃ¼fen (ggf. Feature-Drop)

### 2.3 Modelltraining robuster machen (1â€“2 Tage)
- `src/model_training.py`
- Cross-Validation (z.â€¯B. Stratified K-Fold)
- Vergleich: LightGBM, CatBoost, XGBoost

---

## ğŸŒŸ Phase 3 â€“ PrÃ¤sentation & Clean Notebook (2â€“3 Tage)

### 3.1 Finale Notebook-Version (1 Tag)
- `final_model.ipynb` mit Markdown-Kommentaren
- Kein Herumprobieren mehr, klarer Ablauf:
  - Daten laden â†’ Features â†’ Training â†’ SHAP â†’ Eval

### 3.2 SHAP Analyse (0.5â€“1 Tag)
- `src/shap_analysis.py`
- SHAP Summary Plot, Waterfall fÃ¼r einzelne FÃ¤lle
- Insights (z.â€¯B. â€Top 5 Einflussfaktorenâ€œ)

### 3.3 README finalisieren (0.5 Tag)
- Projektbeschreibung
- Architekturdiagramm (optional)
- Finaler Score
- 1 Screenshot (z.â€¯B. SHAP-Plot)
- Learnings & Herausforderungen

---

## ğŸ” Phase 4 â€“ Advanced Features & Modeling (2â€“3 Tage)

### 4.1 Stacking Classifier (1 Tag)
- `sklearn.ensemble.StackingClassifier`
- Kombiniere LightGBM + CatBoost + XGBoost
- Logistic Regression als Meta-Layer

### 4.2 Meta-Features / Aggregationen (0.5â€“1 Tag)
- z.â€¯B. Credit-Summen pro Antragsteller (bureau.csv)
- ZahlungsverspÃ¤tungen zÃ¤hlen
- Ziel: Mehr Kontext pro Kunde

### 4.3 Target Encoding (0.5 Tag)
- Smoothed target encoding fÃ¼r Kategoricals
- Achte auf Fold-Sicherheit (kein Leakage!)

---

## ğŸ› ï¸ Phase 5 â€“ Deployment & Reproduzierbarkeit (2â€“3 Tage)

### 5.1 CLI & Config-System (0.5 Tag)
- `train.py` â†’ Kommandozeilen-Trainingsskript mit `argparse`
- `config.yaml` â†’ Modellparameter, Pfade, CV-Typ

### 5.2 Dockerisierung (1 Tag)
- `Dockerfile` â†’ Image mit Python, AbhÃ¤ngigkeiten, Skripten
- `docker run homecredit` startet `train.py`
- Optional: `Makefile` (train, eval, clean)

### 5.3 GitHub Actions (optional, 0.5 Tag)
- Linter + Testlauf bei jedem Push
- Zeigt ProfessionalitÃ¤t & DevOps-GrundverstÃ¤ndnis

---

## â±ï¸ Zeitplan-Vorschlag (realistisch: 7â€“14 Tage bei 1â€“3 h/Tag)

| Tag | Aufgabe |
|-----|--------|
| 1   | Struktur + Feature Engineering aufteilen |
| 2   | README + Evaluation |
| 3   | CV, Modellvergleich |
| 4   | SHAP + Feature Selection |
| 5   | Final Notebook bauen |
| 6   | Meta-Features + Target Encoding |
| 7   | Stacking einbauen |
| 8   | CLI + Config |
| 9   | Docker |
| 10  | README finalisieren, GitHub Actions (optional) |

---

## ğŸ“š Optional â€“ Weitere Ideen
- Streamlit Web-App fÃ¼r Prediction Demo
- Vergleich mit AutoML (z.â€¯B. H2O AutoML, AutoSklearn)
- Feature Store (z.â€¯B. `feast`, nur zur Ãœbung)
- Save + Load Pipelines mit `joblib` oder `MLflow`

---

## âœ… Zielstatus: GitHub-Portfolio-Projekt

Am Ende hast du:
- Eine strukturierte, saubere ML-Pipeline
- Stacking-Modell mit SHAP-Analyse
- Dockerized CLI-Skript
- Beeindruckendes README
- Einsatz modernster Praktiken (CV, Encoding, Aggregationen, Reproducibility)
